% Chapter Template


\chapter{Method} % Main chapter title

\label{method} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%learning_rate	0.001
%all_params	3,008,562
%trainable_params	3,003,930
%non_trainable_params	4,632
%layers	156
%training_time_1_epoch	85.43s
%description	Test adam optimizer on hrnet v7

\vspace*{\fill}
\begin{figure}[th]
    \centering
    \includegraphics[width=150mm]{Figures/custom_hrnet_lines.png}
    \decoRule
    \caption[HRNetV3: Our HRNet Architecture]{HRNetV3: Our created High-to-low resolution network architecture}
    \label{fig:custom_hrnet}
\end{figure}
For our end-to-end keypoint recognition architecture we have created three modules to extract the background,
find the body parts in the image and detect the human joints or keypoints.
However, to extract the background was not important for other keypoint recognition architectures such as
\textit{OpenPose}~\cite{openpose} or \textit{VideoPose3D}~\cite{videopose3d}, it is important in our architecture, since we wanted
this recognition architecture to be able to be used during practice when there are multiple skaters on the ice,
but only track the focused skater.
With the body part detection module, we altered the well-established approach from \textit{OpenPose} not to recognize vector
fields, which the joints connect, but the visible body parts.
We assume this method to work seamlessly, and estimate that this approach was not chosen before due to the missing
accurate labels for body part detection.
For the keypoint recognition module, we calculate the gaussian with a radius of three pixels and a standard deviation of
three.
In \autoref{fig:alena_step_labeled} we demonstrate an example frame labeled by our three modules
showing Alena Kostornaia, the 2020 European champion during her program.

\begin{figure}
    \centering
    \includegraphics[width=130mm]{Figures/alena_step_labeled2.png}
    \decoRule
    \caption[HRNetV3: Predictions from Alena Kostornaia's Steps]{Learned labels by the three modules: extracted background, human part detection and
    keypoint detection \textit{skater: Alena Kostornaia, 2020 European champion\cite{2020european}}}
    \label{fig:alena_step_labeled}
\end{figure}


All in all, we have built a fully convolutional architecture with three networks, that all are based on high-to-low
representation learning. Our architecture consists of one input block $\mathcal{N}_I$  and three subsequent blocks
$\mathcal{N}_L$, $\mathcal{N}_M$,$\mathcal{N}_S$ and $\mathcal{N}_{XS}$.
These subsequent blocks combine feature maps with lower coarser representations with the original sized input image
feature maps and thus learn the features of the different levels equal to the HRNet strategy~\cite{HRNetv2, HRNetv1}.
\\\mbox{}\\
However, different from the HRNetV1 and HRNetV2 we do not use Pooling to decrease or increase the size of the feature
maps. We decrease the feature maps with usual convolutions and associate strides $\mathtt{s}$ and kernel sizes
$\mathtt{k}$, with $\mathtt{s}=\mathtt{k}$.
To increase the feature maps we use transposed convolutions with again $\mathtt{s}=\mathtt{k}$ according to the
strided down convolutions.
This allows the network to learn additional weights for the upward and downward convolutions and improve these level
exchange processes.
\\\mbox{}\\
Furthermore, we fuse the layer blocks in the first and second stage to combine the mentioned feature levels.
In the third stage, however, we concatenate all feature levels to fully exploit the multi-resolution convolutions
as argued in HRNetV2~\cite{HRNetv2}.
\\\mbox{}\\
As visualized in~\ref{fig:custom_hrnet} we adjusted the number of feature maps for the different blocks.
Moreover, in the first stage, we use only three convolutional layers for one block, in the other stages we use four layers
for the lower levels and only in $\mathcal{N}_L$ we use three convolutional layers for the blocks throughout the network.
\\\mbox{}\\
Another adjustment is, that we use the input image as initial input for all our block levels but the $\mathcal{N}_{XS}$
block, which uses the fused layers of all the other levels as input.
To every stage we add the input block $\mathcal{N}_I$.
\\\mbox{}\\
This network architecture resulted after several experiments and investigations of the estimated feature maps of the
different blocks and stages.
Every block is completed with batch normalization and a \textit{selu} activation function.
The output of the network is predicted by a linear softmax activation function.
For the background-extraction and keypoint detection network we use mean-squared-error as loss function and in the
human part detection network we use a custom loss function~\autoref{Experiments} to optimize the network weights.
In sum, our network comprises 3,008,562 parameters of which 3,003,930 can be learned.
The amount of all layers for one network is 156.\\


% maybe put some images how the different modules did learn

%learning_rate	0.001
%all_params	3,008,562
%trainable_params	3,003,930
%non_trainable_params	4,632
%layers	156
%training_time_1_epoch	85.43s
%description	Test adam optimizer on hrnet v7


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------


\section{Training Performance (human parts)}
\begin{figure}
    \centering
    \includegraphics[width=130mm]{Figures/hp_accuracy_custom_hrnet_v3.png}
    \decoRule
    \caption[HRNetv3 Training Process: Accuracy]{Improving Accuracy of Training HRNetV3}
    \label{fig:hp_accuracy_hrnet_v3}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=130mm]{Figures/hp_loss_custom_hrnet_v3.png}
    \decoRule
    \caption[HRNetv3 Training Process: Loss]{Decreasing Loss  of Training HRNetV3}
    \label{fig:hp_loss_hrnet_v3}
\end{figure}

We trained the human parts module with the Adam optimizer for 5556 episodes, a batch size of 3, and 64 steps per epoch.
One training epoch took on average 85.43 seconds and lasted about 5.5 days.
For optimization, we have built a custom loss function as explained in
\autoref{Experiments} to better deal with the class invariance of the occurring pixel labels.
The \autoref{fig:hp_accuracy_hrnet_v3} shows, that the accuracy of our network
\autoref{fig:hp_accuracy_hrnet_v3} rises very steep until the 500th episode and then flattens more and more until the last
episode, where the network reaches an excellent accuracy level of 0.99.
In detail, we divided our data into a distinct 90 percent train and 10 percent test dataset.
The figure shows equal trends for both datasets, which means the network did not overfit on the data.\

The loss function in \autoref{fig:hp_loss_hrnet_v3} shows equal opposite trends to the accuracy graph.
It steeply decreases until the 500th episode and then starts to flatten.
Again train and test datasets do not cross each other but show similar courses.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Inference Runtime Analysis}
The time to predict one frame of size 640x480 pixel takes about 0.3 seconds to run on a Quad-Core Intel i7 CPU with 2.20 GHz,
a simple laptop cpu.
Predictions and training can run on simple hardware as the above mentioned CPU.
The training speed will increase if the minimum version of Cuda
3.5 is supported by the system, since then TensorFlow 2 is able to run on the GPU instead of the CPU.
Due to the age of the named laptop Cuda 3.0 was the highest to be supported.
Since recent developments targeting AI chips in the mobile world by the common companies Apple, Samsung or Huawei,
we are very confident that the inference of our network does work as well on commonly sold mobile phones today~\cite{mobileAI}.
This sets apart our architecture from \textit{OpenPose}, which we could not use for inference on the mentioned laptop.

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
\section{Implementation Details}
We build our Network upon the high-level API TensorFlow 2 which is based on the Keras API~\cite{tensorflow2}.
Our training run on the AI server \textit{J.A.R.V.I.S.} provided by the Stuttgart Media University.
The server contains four Nvidia Titan Xp GPUs with 12 GB RAM and a i7 8-core CPU with 3.2 GHz.
We trained each experiment on a separate GPU. The training was conducted in docker containers using the TensorFlow
\textit{latest-devel-gpu}~\cite{tensorflowdocker} docker build.
The latest TensorFlow docker container did not support Python 3 at the time of this research, but only Python in version
2, which is why we had to use the before mentioned descendant.


