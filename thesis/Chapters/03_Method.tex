% Chapter Template


\chapter{Method} % Main chapter title

\label{method} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%learning_rate	0.001
%all_params	3,008,562
%trainable_params	3,003,930
%non_trainable_params	4,632
%layers	156
%training_time_1_epoch	85.43s
%description	Test adam optimizer on hrnet v7

\vspace*{\fill}
\begin{figure}[th]
    \centering
    \includegraphics[width=150mm]{Figures/custom_hrnet_lines.png}
    \decoRule
    \caption[HRNetV3: HRNet Architecture]{HRNetV3: Developed High-to-low resolution network architecture}
    \label{fig:custom_hrnet}
\end{figure}
For the here presented end-to-end keypoint recognition architecture three modules were built to extract the background,
find the body parts in the image and detect the human joints.
However, even if the background extraction was not important for other keypoint recognition architectures such as
\textit{OpenPose}~\cite{openpose} or \textit{VideoPose3D}~\cite{videopose3d}, it is important in here, since
this recognition architecture should be able to be used during practice when there are multiple skaters on the ice,
but only the focused skater should be recognized.
With the body part detection module, the well-established approach from \textit{OpenPose} was altered
in a way to not recognize vector
fields, which the joints connect, but recognize the visible body parts.
This method can be assumed to work seamlessly, and probably this approach was not chosen before due to the missing
accurate labels for body part detection.
For the keypoint recognition module, a Gaussian with a radius of three pixels and a standard deviation of
three was calculated.
In \autoref{fig:alena_step_labeled} a demonstration shows an example frame labeled by these three modules,
showing Alena Kostornaia, the 2020 European champion during her program.

\begin{figure}
    \centering
    \includegraphics[width=130mm]{Figures/alena_step_labeled2.png}
    \decoRule
    \caption[HRNetV3: Predictions from Alena Kostornaia's Steps]{Learned labels by the three modules: extracted background, human part detection and
    keypoint detection \textit{skater: Alena Kostornaia, 2020 European champion\cite{2020european}}}
    \label{fig:alena_step_labeled}
\end{figure}


All in all, a fully convolutional architecture was built with three networks, that all are based on high-to-low
representation learning. This architecture consists of one input block $\mathcal{N}_I$  and three subsequent blocks
$\mathcal{N}_L$, $\mathcal{N}_M$,$\mathcal{N}_S$ and $\mathcal{N}_{XS}$.
These subsequent blocks combine feature maps with lower coarser representations with the original sized input image
feature maps and thus learn the features of the different levels equal to the HRNet strategy~\cite{HRNetv2, HRNetv1}.
\\\mbox{}\\
However, different from the \textit{HRNetV1} and \textit{HRNetV2} Pooling is not used to decrease or increase the
size of the feature
maps. These changes of receptive fields are conducted with usual convolutions and associate strides $\mathtt{s}$ and
kernel sizes
$\mathtt{k}$, with $\mathtt{s}=\mathtt{k}$.
To increase the feature maps a transposed convolution is applied with again $\mathtt{s}=\mathtt{k}$ according to the
strided down convolutions.
This allows the network to learn additional weights for the upward and downward convolutions and improve these level
exchange processes.
\\\mbox{}\\
Furthermore, the layer blocks in the first and second stage are fused to combine the mentioned feature levels.
In the third stage, however, all feature levels are concatenated to fully exploit the multi-resolution convolutions
as argued in \textit{HRNetV2}~\cite{HRNetv2}.
\\\mbox{}\\
As visualized in~\ref{fig:custom_hrnet} the number of feature maps was adjusted for the different blocks.
Moreover, in the first stage, only three convolutional layers were used for one block, in the other stages four layers
where used for the lower levels and only in $\mathcal{N}_L$ three convolutional layers were applied for the blocks
throughout the network.
\\\mbox{}\\
Another adjustment is, that the input image as initial input was added to all block levels but the $\mathcal{N}_{XS}$
block, which uses the fused layers of all the preceding levels as input.
Every last block of the stages combines the information from all previos block levels and additionally
the input block $\mathcal{N}_I$ was added.
\\\mbox{}\\
This network architecture resulted after several experiments and investigations of the estimated feature maps of the
different blocks and stages.
Every block is completed with batch normalization and a \textit{selu} activation function.
The output of the network is predicted by a linear softmax activation function.
For the background-extraction and keypoint detection network \textit{mean-squared-error} calculates the loss, and in the
human part detection network a custom loss function was developed, as described in~\autoref{experiments}, to optimize
the network's weights.
In sum, the presented network comprises 3,008,562 parameters of which 3,003,930 can be learned.
The amount of all layers for the network is 156.\\


% maybe put some images how the different modules did learn

%learning_rate	0.001
%all_params	3,008,562
%trainable_params	3,003,930
%non_trainable_params	4,632
%layers	156
%training_time_1_epoch	85.43s
%description	Test adam optimizer on hrnet v7


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------


\section{Training Performance (human parts)}
\begin{figure}
    \centering
    \includegraphics[width=130mm]{Figures/hp_accuracy_custom_hrnet_v3.png}
    \decoRule
    \caption[HRNetv3 Training Process: Accuracy]{Improving Accuracy of Training HRNetV3}
    \label{fig:hp_accuracy_hrnet_v3}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=130mm]{Figures/hp_loss_custom_hrnet_v3.png}
    \decoRule
    \caption[HRNetv3 Training Process: Loss]{Decreasing Loss of Training HRNetV3}
    \label{fig:hp_loss_hrnet_v3}
\end{figure}

The human parts module was trained with the Adam optimizer for 5556 episodes, a batch size of 3,
and 64 steps per epoch.
One training epoch took on average 85.43 seconds.
The whole training lasted about 5.5 days.
For optimization, a custom developed loss function was used as will be explained in
\autoref{experiments} to better deal with the class invariance of the occurring pixel labels.
The \autoref{fig:hp_accuracy_hrnet_v3} shows, that the accuracy of the here presented network
\autoref{fig:hp_accuracy_hrnet_v3} rises very steep until the 500th episode and then flattens more and more until the
last
episode, where the network reaches an excellent accuracy level of 0.99.
In detail, the data was divided into a distinct 90 percent train and 10 percent test dataset.
The figure shows equal trends for both datasets, which means the network did not overfit on the data.\

The loss function in \autoref{fig:hp_loss_hrnet_v3} shows equal opposite trends to the accuracy graph.
It steeply decreases until the 500th episode and then starts to flatten.
Again, train and test datasets do not cross each other but show similar courses.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Inference Runtime Analysis}
The time to predict one frame of size 640x480 pixel takes about 0.3 seconds to run on a Quad-Core Intel i7 CPU with 2.20 GHz,
a simple laptop CPU.
Predictions and training can run on simple hardware as the above mentioned CPU.
The training speed will increase if the minimum version of Cuda
3.5 is supported by the system, since then TensorFlow 2 is able to run on the GPU instead of the CPU.
Due to the age of the named laptop, Cuda 3.0 was the highest to be supported.
Since recent developments targeting AI chips in the mobile world by the common companies \textit{Apple, Samsung} or \textit{Huawei},
it is to be expected that the inference of the network does work as well on commonly sold mobile phones today~\cite{mobileAI}.
This sets apart the here presented architecture from \textit{OpenPose}, which couldn't be used for inference on the mentioned laptop.

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
\section{Implementation Details}
The Network was built upon the high-level API TensorFlow 2 which is based on the Keras API~\cite{tensorflow2}.
The training run on the AI server \textit{J.A.R.V.I.S.} provided by the \textit{Stuttgart Media University}.
The server contains four \textit{Nvidia Titan Xp} GPUs with 12 GB RAM and a i7 8-core CPU with 3.2 GHz.
Each experiment was trained on a separate GPU. The training was conducted in docker containers using the TensorFlow
\textit{latest-devel-gpu}~\cite{tensorflowdocker} docker build.
The latest TensorFlow docker container did not support Python 3 at the time of this research, but only Python in version
2, which is why the before mentioned descendant had to be used.


